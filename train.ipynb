{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b100aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb6f609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception is 'HyperParameters' object has no attribute 'hf_access_token'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6fa15b06894e508da241bc8fb51c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinhpx/Workspaces/llm-fp8/te_llama.py:182: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  layer_prefix_pat = \"model.layers.\\d+.\"\n",
      "Supported flash-attn versions are >= 2.1.1, <= 2.7.4.post1. Found flash-attn 2.8.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cache directory : /home/vinhpx/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08\n",
      "Resolved archive file: /home/vinhpx/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinhpx/miniconda3/envs/llm-fp8/lib/python3.12/site-packages/accelerate/accelerator.py:516: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5e18e8d5d144d0b6a87cea29cc477a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fd8c308e5846c1baea6c514faa67ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openassistant_best_replies_train.jsonl:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4395ce4eb246c6925ca033d871e92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openassistant_best_replies_eval.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231dc820007c475cb6e97a227a6bb2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30592bc99e974207b883961cb8169b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a70fa0c4c6473f9ea3ce2621d2ae86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Finetune the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/llm-fp8/utils.py:233\u001b[39m, in \u001b[36mfinetune_model\u001b[39m\u001b[34m(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)\u001b[39m\n\u001b[32m    228\u001b[39m end.record()\n\u001b[32m    229\u001b[39m accelerator.end_training()\n\u001b[32m    231\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams.num_training_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m finetuning steps complete!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAverage time taken per step:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[43mstart\u001b[49m\u001b[43m.\u001b[49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m/hyperparams.num_training_steps)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m milliseconds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-fp8/lib/python3.12/site-packages/torch/cuda/streams.py:216\u001b[39m, in \u001b[36mEvent.elapsed_time\u001b[39m\u001b[34m(self, end_event)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34melapsed_time\u001b[39m(\u001b[38;5;28mself\u001b[39m, end_event):\n\u001b[32m    211\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return the time elapsed.\u001b[39;00m\n\u001b[32m    212\u001b[39m \n\u001b[32m    213\u001b[39m \u001b[33;03m    Time reported in milliseconds after the event was recorded and\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    before the end_event was recorded.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_event\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages, methods and variables\n",
    "from utils import *\n",
    "\n",
    "hyperparams.model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "hyperparams.weights_cache_dir = \"/home/vinhpx/.cache/huggingface/hub\"\n",
    "hyperparams.mixed_precision = \"bf16\"\n",
    "\n",
    "\n",
    "# Init the model and accelerator wrapper\n",
    "model = init_te_llama_model(hyperparams)\n",
    "accelerator, model, optimizer, train_dataloader, lr_scheduler = wrap_with_accelerator(model, hyperparams)\n",
    "\n",
    "\n",
    "# Finetune the model\n",
    "finetune_model(model, hyperparams, accelerator, train_dataloader, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78866ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-fp8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
