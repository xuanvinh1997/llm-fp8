\begin{frame}{Experiment and analysis: Memory efficiency}

    \textbf{VRAM usage comparison}
    
    \centering
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{BF16} & \textbf{Ours} & \textbf{Delta} \\
    \midrule
    Llama-3.2-3B & 82.4 GB & \textbf{74.2 GB} & \textcolor{blue}{-10.0\%} \\
    Llama-3.1-8B & 80.0 GB & 88.3 GB & \textcolor{red}{+10.4\%} \\
    \bottomrule
    \end{tabular}

    \vspace{0.8cm}

    \textbf{Analysis of 8B memory overhead}
    
    \small
    While 3B shows expected savings, 8B shows an increase due to:
    \begin{itemize}
        \item \textbf{Workspace buffers}: Fixed-size buffers for FP8 casting operations
        \item \textbf{Scaling metadata}: Storing history/amax for dynamic scaling
    \end{itemize}
    
    \vspace{0.3cm}
    \textit{Note}: The \textbf{27\% speedup} outweighs this memory cost for training throughput
\end{frame}
