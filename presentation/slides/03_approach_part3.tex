\begin{frame}[fragile]{Implementation: Custom Transformer Engine Layer}
    \begin{block}{Dynamic Layer Replacement}
    \small We implemented a custom \texttt{TELlamaDecoderLayer} to inject FP8 logic while preserving weights:
    \end{block}
    \vspace{0.2cm}
    
    \begin{minted}[fontsize=\tiny]{python}
class TELlamaDecoderLayer(torch.nn.Module):
    def __init__(self, config, ...):
        # Replace standard Attention with TE FP8-aware module
        self.self_attention = te.pytorch.MultiheadAttention(
            hidden_size=config.hidden_size,
            ...
        )
        # Replace MLP with TE LayerNormMLP
        self.layernorm_mlp = te.pytorch.LayerNormMLP(
            activation="swiglu", ...
        )

# Context manager for runtime patching
@contextmanager
def replace_decoder(te_decoder_cls):
    original_cls = LlamaDecoderLayer
    LlamaDecoderLayer = te_decoder_cls
    yield
    LlamaDecoderLayer = original_cls
    \end{minted}

    \begin{itemize}
        \item \textbf{Runtime Integration}: Dynamic patching of Hugging Face architectures
        \item \textbf{Format Management}: Automated mixed-precision control via \texttt{fp8\_autocast}
    \end{itemize}
\end{frame}
