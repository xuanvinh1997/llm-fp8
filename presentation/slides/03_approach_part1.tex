\begin{frame}{Proposed Approach: Layer-Wise FP8 Specialization}

\textbf{Key Observation:} Transformer components exhibit distinct numerical characteristics requiring tailored precision-range allocation

\vspace{0.4cm}

\textbf{Methodology: Component-Specific Format Assignment}

\small
\begin{itemize}
    \item \textbf{Attention Layers}: Hybrid E4M3/E5M2
    \begin{itemize}
        \small
        \item Forward: \textbf{E4M3} (bounded activations)
        \item Backward: \textbf{E5M2} (extended range for gradient amplification)
    \end{itemize}
    \item \textbf{MLP Layers}: E4M3 only
    \begin{itemize}
        \small
        \item Forward/Backward: \textbf{E4M3}
        \item Narrow distribution $\rightarrow$ precision prioritized
    \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Theoretical Foundation}

\textbf{Attention: E5M2 Assignment}

\small
\textit{Rationale: Softmax Gradient Amplification}
\begin{itemize}
    \item Softmax gradients exhibit extreme dynamic range
    \item \textbf{E5M2} (max $\sim$57,000): Prevents overflow in backward pass
    \item \textbf{E4M3} (max $\sim$448): Insufficient for gradient magnitudes
\end{itemize}

\vspace{0.5cm}

\textbf{MLP: E4M3 Assignment}

\small
\textit{Rationale: Precision-Dominant Requirement}
\begin{itemize}
    \item Feed-forward networks exhibit concentrated value distributions
    \item \textbf{E4M3}: Higher mantissa precision minimizes quantization error
    \item Critical for maintaining convergence accuracy
\end{itemize}

\end{frame}
