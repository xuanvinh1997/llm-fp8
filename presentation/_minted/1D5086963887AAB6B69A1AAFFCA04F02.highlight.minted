\begin{MintedVerbatim}[commandchars=\\\{\}]
\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{TELlamaDecoderLayer}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{config}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Replace standard Attention with TE FP8\PYGZhy{}aware module}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{self\PYGZus{}attention} \PYG{o}{=} \PYG{n}{te}\PYG{o}{.}\PYG{n}{pytorch}\PYG{o}{.}\PYG{n}{MultiheadAttention}\PYG{p}{(}
            \PYG{n}{hidden\PYGZus{}size}\PYG{o}{=}\PYG{n}{config}\PYG{o}{.}\PYG{n}{hidden\PYGZus{}size}\PYG{p}{,}
            \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
        \PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Replace MLP with TE LayerNormMLP}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{layernorm\PYGZus{}mlp} \PYG{o}{=} \PYG{n}{te}\PYG{o}{.}\PYG{n}{pytorch}\PYG{o}{.}\PYG{n}{LayerNormMLP}\PYG{p}{(}
            \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{swiglu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
        \PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Context manager for runtime patching}
\PYG{n+nd}{@contextmanager}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{replace\PYGZus{}decoder}\PYG{p}{(}\PYG{n}{te\PYGZus{}decoder\PYGZus{}cls}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{original\PYGZus{}cls} \PYG{o}{=} \PYG{n}{LlamaDecoderLayer}
    \PYG{n}{LlamaDecoderLayer} \PYG{o}{=} \PYG{n}{te\PYGZus{}decoder\PYGZus{}cls}
    \PYG{k}{yield}
    \PYG{n}{LlamaDecoderLayer} \PYG{o}{=} \PYG{n}{original\PYGZus{}cls}
\end{MintedVerbatim}
