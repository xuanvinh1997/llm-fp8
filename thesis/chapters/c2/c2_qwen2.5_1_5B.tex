\section{Llama Architecture}
Llama 3 is a family of decoder-only transformer language models developed by Meta AI, ranging from 1B to 405B parameters. The Llama 3.1 and 3.2 variants used in our experiments share a consistent architectural design that combines grouped-query attention (GQA), SwiGLU feed-forward networks, and RMSNorm normalization.

Table~\ref{tab:llama-sizes} summarizes the model configurations evaluated in this work. Both models use the same fundamental architecture with different parameter scales and training characteristics.

\begin{table}[htbp]
\centering
\caption{Key hyper-parameters for Llama 3.2 and 3.1 model configurations.}
\label{tab:llama-sizes}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
Model & Layers & $d_{\text{model}}$ & Attention Heads & Parameters & Context \\
\midrule
Llama-3.2-3B  & 32 & 3\,072 & 32 & 3.21 B & 8\,192 \\
Llama-3.1-8B  & 32 & 4\,096 & 32 & 8.03 B & 128\,000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Architectural Components}

\textbf{Tokenization:} Llama 3 employs a byte-pair encoding (BPE) tokenizer with a vocabulary size of 128K tokens, optimized for multilingual text and code. The tokenizer ensures efficient encoding across diverse languages while maintaining compatibility with the model's context window.

\textbf{Positional Encodings:} Positional information is injected via Rotary Position Embeddings (RoPE), which apply rotation matrices to query and key vectors in the attention mechanism. RoPE enables better extrapolation to longer sequences and provides improved relative position modeling compared to absolute positional encodings.

\textbf{Attention Mechanism:} Each decoder block uses grouped-query attention (GQA), where multiple query heads share a smaller number of key-value heads. This design reduces the memory footprint of the key-value cache during inference while maintaining model quality. For Llama-3.2-3B and 3.1-8B, the standard multi-head attention configuration is used with 32 heads.

\textbf{Feed-Forward Networks:} The classical two-layer MLP is replaced by a SwiGLU gated-activation block:
\[
\mathrm{MLP}(x)\;=\;W_\text{down}\bigl[\operatorname{SiLU}(xW_\text{gate})
\odot(xW_\text{up})\bigr].
\]
SwiGLU provides superior performance compared to standard activation functions while maintaining parameter efficiency through gating mechanisms.

\textbf{Normalization:} RMSNorm is applied before each sub-layer (pre-normalization), including before self-attention and feed-forward blocks. Compared with LayerNorm, RMSNorm avoids mean subtraction, reducing computational cost and improving numerical stability in mixed-precision trainingâ€”a crucial property for FP8 optimization.

\subsection{Block Formula}
A single Llama decoder layer computes:
\begin{align}
h' &= h + \operatorname{Attn}\!\bigl(\operatorname{RMSNorm}(h)\bigr), \\
h  &= h' + \operatorname{SwiGLU}\!\bigl(\operatorname{RMSNorm}(h')\bigr).
\end{align}

This architecture provides an ideal testbed for layer-wise FP8 format assignment due to its clear separation between attention mechanisms (which benefit from E5M2's dynamic range) and feed-forward networks (which benefit from E4M3's precision).

