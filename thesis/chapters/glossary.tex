\chapter*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}
 
\begin{tabular}{|p{0.6cm}|p{2.0cm}|p{12.0cm}|}
% \begin{tabular}{|m{0.6cm}|m{2cm}|m{6cm}|m{6cm}|}
    \hline
\thead{No.} & \thead{Term} & \thead{Definition}  \\ 
                    \hline
1 & LLM & Large Language Model \\ \hline
2 & GPU & Graphics Processing Unit \\ \hline
3 & FP8 & 8-bit Floating Point \\ \hline
4 & E4M3 & Exponent-4-Mantissa-3: \\ \hline
5 & E5M2 & Exponent-5-Mantissa-2 \\ \hline
6 & BF16 & Brain Floating Point \\ \hline
7 & FP16 & 16-bit Floating Point \\ \hline
8 & FP32 & 32-bit Floating Point \\ \hline
9 & TE & Transformer Engine library that enables the automatic use of FP8 for performant matrix multiplications while maintaining higher precision for other operations to preserve numerical stability. \\ \hline
10 & Qwen2.5 & A series of transformer-based language models developed by Alibaba Cloud, known for their efficiency and strong performance across general and domain-specific tasks. \\ \hline
11 & GQA & Grouped Query Attention \\ \hline
12 & KV Caching & Key-Value Caching \\ \hline
13 & vLLM & A high-throughput and memory-efficient inference engine for LLMs that implements PagedAttention, KV caching, and other optimizations for faster text generation. \\ \hline
\end{tabular}