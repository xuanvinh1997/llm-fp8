\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This thesis investigates the optimization of Large Language Model training through layer-wise FP8 format assignment, addressing the critical challenge of memory and computational efficiency in modern transformer architectures. While FP8 precision offers significant potential for reducing resource requirements, existing approaches predominantly employ uniform format assignment strategies that fail to leverage the complementary strengths of available FP8 variants—E4M3 (4-bit exponent, 3-bit mantissa) and E5M2 (5-bit exponent, 2-bit mantissa).

We propose a systematic layer-wise FP8 format specialization strategy that optimally assigns formats based on the distinct computational characteristics of different transformer components. Our approach assigns E4M3 to multi-layer perceptrons (MLPs) to leverage higher mantissa precision for stable feed-forward computations, while employing E5M2 for attention query-key operations that require extended exponent ranges for handling dynamic activation patterns. We implement this methodology using NVIDIA's Transformer Engine framework and validate it through comprehensive experiments on Llama-3.2-3B and Llama-3.1-8B models trained on 100K samples from the OpenMathInstruct-2 corpus.

Our experimental results demonstrate that layer-wise FP8 optimization achieves substantial improvements across multiple dimensions. For the 3B model, we obtain 10.0\% memory reduction (82.4→74.2 GB) and 42\% faster training time (3.6h→2.1h) compared to BF16 baseline. For the 8B model, our approach delivers 27\% training time reduction (9.1h→6.6h) while outperforming hybrid FP8 implementations. Most significantly, our method achieves exceptional numerical stability with loss variance consistently below 0.4, representing approximately 50\% lower variance compared to hybrid FP8 approaches that exhibit periodic instability spikes reaching 0.8+ throughout training.

The thesis contributes to the field in three significant ways: First, it provides comprehensive analysis of computational patterns across transformer components, identifying optimal FP8 format assignments based on numerical characteristics. Second, it presents detailed implementation methodology for layer-wise format assignment that can be extended to other transformer architectures. Third, it offers extensive experimental validation demonstrating consistent improvements in memory efficiency, training throughput, and numerical stability. This work advances the practical training of large language models by enabling more efficient resource utilization while maintaining model quality.

\textbf{Keywords}: \textit{Large Language Models}, \textit{FP8 Precision}, \textit{Mixed Precision Training}, \textit{Transformer Optimization}, \textit{Layer-wise Format Assignment}, \textit{Numerical Stability}