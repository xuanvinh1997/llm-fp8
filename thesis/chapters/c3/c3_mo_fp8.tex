% ---------- Training with FP8 ----------
\section{Layer-Wise FP8 Training Methodology}

\subsection{Hardware Platform}
All experiments were executed on NVIDIA Blackwell architecture GPUs with 96 GB of memory, leveraging advanced FP8 Tensor Core capabilities. The hardware provides native support for both E4M3 and E5M2 formats, which we access through the NVIDIA Transformer Engine (TE) library\cite{transformer_engine}.

\subsection{Training Configuration}
\vspace{-0.5em}
\begin{table}[htbp]
    \centering\small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Configuration} & \textbf{Llama-3.2-3B} & \textbf{Llama-3.1-8B} \\
        \midrule
        Batch Size (BF16) & 24 & 6 \\
        Batch Size (FP8) & 24 & 6 \\
        Sequence Length & 512 & 512 \\
        Learning Rate & 1e-5 & 1e-5 \\
        Epochs & 3 & 3 \\
        Training Samples & 100K & 100K \\
        Optimizer & AdamW & AdamW \\
        Gradient Accumulation & 4 & 4 \\
        \bottomrule
    \end{tabular}
    \caption{Training configuration across model scales.}
    \label{tab:fp8_config}
\end{table}

\subsection{Converting Pre-trained Models to Transformer Engine}
We convert Llama model checkpoints from Hugging Face by replacing the standard \texttt{LlamaDecoderLayer} with a custom \texttt{TELlamaDecoderLayer} that uses Transformer Engine components. The conversion is performed using a context manager that temporarily patches the Llama architecture:

\begin{verbatim}
@contextmanager
def replace_decoder(te_decoder_cls):
    original_cls = transformers.models.llama.modeling_llama.LlamaDecoderLayer
    transformers.models.llama.modeling_llama.LlamaDecoderLayer = te_decoder_cls
    try:
        yield
    finally:
        transformers.models.llama.modeling_llama.LlamaDecoderLayer = original_cls
\end{verbatim}

The \texttt{TELlamaDecoderLayer} replaces standard PyTorch layers with Transformer Engine equivalents:

\begin{verbatim}
class TELlamaDecoderLayer(torch.nn.Module):
    def __init__(self, config, *args, **kwargs):
        super().__init__()
        # Replace attention with TE MultiheadAttention
        self.self_attention = te.pytorch.MultiheadAttention(
            hidden_size=config.hidden_size,
            num_attention_heads=config.num_attention_heads,
            num_gqa_groups=config.num_key_value_heads,
            normalization="RMSNorm",
            ...
        )
        # Replace MLP with TE LayerNormMLP
        self.layernorm_mlp = te.pytorch.LayerNormMLP(
            hidden_size=config.hidden_size,
            ffn_hidden_size=config.intermediate_size,
            normalization="RMSNorm",
            activation="swiglu"
        )
\end{verbatim}

This approach preserves the original model weights while enabling layer-wise FP8 format control through Transformer Engine's specialized modules.

\subsection{Layer-Wise Format Assignment Implementation}\label{sec:precision_recipes}

Our layer-wise FP8 approach assigns formats based on component type, compared against two baselines:

\textbf{BF16 Baseline:}
Standard mixed-precision training where weights, activations, and gradients remain in BF16, with FP32 optimizer states. This serves as our accuracy and performance reference.

\textbf{Hybrid FP8:}
NVIDIA's uniform delayed scaling approach where weights and forward activations use E4M3 format, while gradient buffers use E5M2, applied uniformly across all layers. The optimizer maintains FP32 master weights.

\textbf{Our Layer-wise FP8:}
Selective format assignment based on component characteristics:
\begin{itemize}
\item \textbf{MLP layers:} All weights, activations, and gradients use E4M3 for stable, high-precision computation
\item \textbf{Attention Q/K projections:} Use E5M2 for extended dynamic range in query-key operations
\item \textbf{Attention V/O projections:} Use E4M3 for precision in value and output computations
\end{itemize}
The optimizer maintains FP32 master weights across all configurations to ensure numerical stability.

\subsection{Dynamic Scaling Mechanism}
To maintain numerical stability during FP8 training, we implement a dynamic scaling mechanism that adapts to the numerical range of tensors. This is critical because the limited range of FP8 would otherwise lead to frequent overflow or underflow.

\begin{equation}
    \text{FP8}_{\text{tensor}} = \text{round}(\text{tensor} \times \text{scale\_factor})
\end{equation}

The scale factors are computed using a combination of techniques:

\begin{itemize}
    \item \textbf{History-based scaling}: Scale factors are derived from an exponential moving average of recent tensor statistics, with varying window sizes (128 for forward passes, 64 for backward gradients).
    \item \textbf{Margin-based amax calculation}: Maximum absolute values (\textit{amax}) are multiplied by a margin factor (1.1 for weights, 1.3 for activations) to provide headroom for outliers.
    \item \textbf{Format-specific scaling}: Different scaling strategies are applied to E4M3 and E5M2 formats based on their representable ranges.
\end{itemize}

\subsection{Implementation of FP8 Matrix Multiplications}

For each matrix multiplication operation, we implement the following sequence:

\begin{enumerate}
    \item \textbf{Precompute scale factors}: Before training begins, we perform a calibration pass on a small batch of data to initialize scale factors.
    \item \textbf{Cast to FP8}: During each forward pass, tensors are converted to FP8 using the current scale factors:
    \begin{verbatim}
    fp8_weight = cast_to_fp8(weight * weight_scale)
    fp8_input = cast_to_fp8(input * input_scale)
    \end{verbatim}
    
    \item \textbf{Perform FP8 matrix multiplication}: The multiplication is executed using Tensor Cores' FP8 capability:
    \begin{verbatim}
    fp8_output = matmul(fp8_input, fp8_weight)
    \end{verbatim}
    
    \item \textbf{Dequantize the result}: The result is scaled back to higher precision:
    \begin{verbatim}
    output = fp8_output / (input_scale * weight_scale)
    \end{verbatim}
    
    \item \textbf{Update scale factors}: After each iteration, scale factors are updated based on observed tensor statistics.
\end{enumerate}

\subsection{Numerical Stability Considerations}\label{sec:numerical_stability}

Several techniques were employed to maintain numerical stability with FP8:

\begin{itemize}
    \item \textbf{Layer-specific scaling}: Different scale factors are computed for each layer to accommodate varying numerical distributions.
    \item \textbf{Operation selectivity}: Only matrix multiplications use FP8, while other operations such as activations, additions, and normalizations remain in higher precision (BF16).
    \item \textbf{Delayed scaling updates}: Scale factors are updated every 16 iterations to prevent oscillations.
    \item \textbf{Gradient clipping}: We apply a global norm gradient clipping of 1.0 to prevent extreme gradient values.
    \item \textbf{FP32 master weights}: The optimizer maintains a full-precision copy of weights to accumulate small gradient updates that would be lost in lower precision.
\end{itemize}

\subsection{Layer-Wise FP8 Recipe Configuration}

We configure different FP8 recipes for attention and MLP layers based on our layer-wise format assignment strategy. The implementation uses Transformer Engine's DelayedScaling mechanism with format-specific configurations:

\begin{verbatim}
from transformer_engine.common.recipe import Format, DelayedScaling

# Attention layers: Use HYBRID format (E4M3/E5M2)
attn_recipe = DelayedScaling(
    fp8_format=Format.HYBRID,
    amax_history_len=16,
    amax_compute_algo="max"
)

# MLP layers: Use E4M3 format exclusively
mlp_recipe = DelayedScaling(
    fp8_format=Format.E4M3,
    amax_history_len=16,
    amax_compute_algo="max"
)
\end{verbatim}

During the forward pass, we apply format-specific recipes to each component:

\begin{verbatim}
# Attention with HYBRID format (E5M2 for Q/K)
with te.pytorch.fp8_autocast(enabled=True, fp8_recipe=attn_recipe):
    attn_out = self.self_attention(hidden_states, ...)

# MLP with E4M3 format
with te.pytorch.fp8_autocast(enabled=True, fp8_recipe=mlp_recipe):
    ffn_out = self.layernorm_mlp(hidden_states)
\end{verbatim}

This configuration assigns HYBRID format to attention operations (allowing E5M2 for query-key projections with extended dynamic range) while MLP layers exclusively use E4M3 for stable, high-precision computation.

% ---------- End of section ----------
