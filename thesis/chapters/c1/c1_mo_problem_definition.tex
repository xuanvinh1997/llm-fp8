\section{Motivation}\label{sec:motivation}

The fundamental challenge in training large language models lies in the computational and memory costs associated with scaling to billions of parameters. Modern transformer-based LLMs achieve higher accuracy primarily by scaling up parameter counts and training data, but this leads to massive resource requirements that strain even data-center GPU capabilities. Training a multi-billion parameter model requires not only loading all weights into GPU memory but also maintaining activations, gradients, and optimizer states throughout the training process.

\subsection{The Challenge of FP8 Format Selection}

NVIDIA's standardization of two FP8 formats presents both opportunities and challenges. E4M3 provides higher mantissa precision within a limited range, making it suitable for stable, dense computations. Conversely, E5M2 offers broader exponent coverage at the cost of mantissa resolution, better suited for operations with wide dynamic ranges.

Existing FP8 training approaches face a critical limitation: they predominantly employ uniform format assignment strategies. For instance, recent implementations apply E4M3 universally across all transformer components. However, our analysis reveals that different transformer components exhibit distinct computational patterns:

\textbf{MLP Stability.} Feed-forward (MLP) layers exhibit concentrated activation and gradient distributions, largely confined within moderate ranges. Their dense matrix multiplications evolve smoothly during training, benefiting from the higher mantissa precision of E4M3.

\textbf{Attention Variability.} Self-attention layers display wider dynamic ranges, especially in query-key interactions. The softmax operation amplifies scale sensitivity, and attention weights often span several orders of magnitude, motivating the use of E5M2's extended exponent range.

\textbf{Gradient Amplification.} Backpropagation through attention further increases gradient variance, particularly in query and key projections. The chain rule propagation multiplies scaling factors, producing distributions broader than those in MLP layers, requiring sufficient exponent headroom.

Our objective is therefore to design a systematic layer-wise FP8 format assignment strategy that: \emph{(i)} leverages the complementary strengths of E4M3 and E5M2 formats, \emph{(ii)} assigns formats based on component-specific computational patterns, and \emph{(iii)} achieves superior memory efficiency, training throughput, and numerical stability compared to uniform format approaches.
