\section{Pruning Methodologies for Model Compression}

Pruning is a family of techniques aimed at reducing model size and computational cost by removing redundant weights. This is typically achieved by setting a fraction of the weights to zero, resulting in a sparse model that can be executed more efficiently.

\subsection{SparseGPT: A One-Shot Pruning Framework}

SparseGPT \cite{frantar2023sparsegpt} is a one-shot pruning method, meaning it induces sparsity in a pre-trained model without requiring any subsequent fine-tuning. It is designed to efficiently find an optimal sparse representation of the weights that minimizes the impact on the model's output.

\subsubsection{Problem Formulation}
The core objective of SparseGPT is, for a given pre-determined sparsity mask $M$, to find the optimal values for the remaining weights that minimize the layer's output reconstruction error. This can be expressed as:

\begin{equation}
\underset{\mathbf{W}_M}{\text{argmin}} \| \mathbf{W}X - \mathbf{W}_M X \|^2_2
\end{equation}

Here, $\mathbf{W}_M$ is the pruned weight matrix, which is constrained to have zeros at the positions indicated by the mask $M$.

\subsubsection{The Challenge of Row-wise Hessian Computation}
A significant challenge in solving this optimization problem is that each row of the weight matrix $\mathbf{W}$ can have a unique sparsity pattern. Consequently, a direct application of second-order optimization methods would require computing a different masked Hessian inverse, $(\mathbf{H}_F)^{-1}$, for each row. For models with billions of parameters, this is computationally infeasible.

\subsubsection{The SparseGPT Solution: Sequential Column Pruning}
SparseGPT circumvents this issue with a key insight: by processing the weights in a column-by-column fashion, it is possible to reuse Hessian information across different rows. The algorithm proceeds as follows: when a weight in column $j$ and row $i$ is pruned, the optimal update for the remaining weights in columns greater than $j$ depends only on the sub-Hessian corresponding to those columns. This sub-Hessian is shared across all rows, irrespective of their individual sparsity patterns. By updating only the weights to the "right" of the current column, SparseGPT can efficiently propagate the error from pruning while reusing the same Hessian information.

\subsubsection{Algorithmic Description}
\begin{algorithm}[H]
\caption{SparseGPT Algorithm}
\begin{algorithmic}[1]
\Require Weight matrix $\mathbf{W}$, inverse Hessian $\mathbf{H}_{\text{inv}}$, target sparsity $p$
\Ensure Sparse weight matrix $\mathbf{W}_{\text{sparse}}$
\State Precompute the Cholesky decomposition of the inverse Hessian, $\mathbf{H}_{\text{inv}}$.
\For{each column $j$ from 0 to $d_{\text{col}}$}
    \For{each row $i$}
        \State Decide whether to prune the weight $w_{ij}$ based on a selection criterion.
        \If{the weight is pruned}
            \State Set $w_{ij} = 0$.
        \EndIf
        \State Compute the error introduced by pruning: $\text{err} = (w_{ij,\text{original}} - w_{ij}) / [\mathbf{H}_{\text{inv}}]_{jj}$.
        \State Update the remaining weights in the row to compensate for this error: $\mathbf{W}[i, j+1:] \leftarrow \mathbf{W}[i, j+1:] - \text{err} \cdot \mathbf{H}_{\text{inv}}[j, j+1:]$.
    \EndFor
\EndFor
\State \textbf{return} $\mathbf{W}_{\text{sparse}}$
\end{algorithmic}
\end{algorithm}

\subsubsection{Adaptive Mask Selection}
Instead of using a fixed, pre-determined mask, SparseGPT can adaptively select which weights to prune. This is done by evaluating the error that would be introduced by pruning each weight and then removing those that cause the least error. The criterion, derived from the Optimal Brain Surgeon framework \cite{hassibi1993optimal}, is:
\begin{equation}
\epsilon_{ij} = \frac{|w_{ij}|^2}{[\mathbf{H}_{\text{inv}}]_{jj}}
\end{equation}
Weights with the smallest $\epsilon_{ij}$ are pruned until the desired target sparsity is reached.

\subsubsection{Support for Semi-Structured Sparsity}
SparseGPT can also be adapted to generate semi-structured sparsity patterns, such as the N:M pattern (e.g., 2:4 sparsity), which are often required for hardware acceleration. In this case, the selection process is constrained: for each group of M consecutive weights, the N weights with the lowest pruning error are preserved, and the rest are pruned.

\subsubsection{Computational Complexity}
The complexity of SparseGPT is dominated by the Hessian preparation, which is $O(d_{\text{col}}^3)$, and the pruning pass itself, which is $O(d_{\text{row}} \cdot d_{\text{col}}^2)$.

\subsection{Wanda: A Simplified Pruning Criterion}

Wanda (short for Weights AND Activations) \cite{sun2023wanda} is another one-shot pruning method that offers a much simpler and faster alternative to SparseGPT.

\subsubsection{The Wanda Importance Score}
The core idea of Wanda is to prune weights based on a simple importance score that considers both the magnitude of the weight and the magnitude of its corresponding input activation. The importance of a weight $w_{ij}$ is defined as:
\begin{equation}
\text{Importance}(w_{ij}) = |w_{ij}| \cdot \|X[:,j]\|_2
\end{equation}
Here, $\|X[:,j]\|_2$ is the L2-norm of the $j$-th input feature's activations, computed over a calibration dataset. This metric identifies weights that have a large magnitude and are also applied to inputs that are frequently large, making them more influential.

\subsubsection{Pruning without Reconstruction}
Unlike SparseGPT, Wanda does not perform any weight updates to compensate for the pruning error. It simply sets the weights with the lowest importance scores to zero. This simplification makes Wanda significantly faster—often completing in minutes rather than hours—while still achieving competitive results, particularly at moderate levels of sparsity (e.g., 50\%).
