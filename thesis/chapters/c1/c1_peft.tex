\section{Parameter-Efficient Fine-Tuning Methodologies}

Full fine-tuning of large language models is often computationally prohibitive due to the immense memory required to store the model's weights, gradients, and optimizer states. Parameter-Efficient Fine-Tuning (PEFT) methods address this by updating only a small subset of the model's parameters, significantly reducing the resource requirements.

\subsection{LoRA: Low-Rank Adaptation}

\subsubsection{Mathematical Foundation}
The Low-Rank Adaptation (LoRA) technique \cite{hu2021lora} is motivated by the hypothesis that the change in weights during model adaptation has a low "intrinsic rank." This suggests that the weight updates can be effectively represented in a much lower-dimensional subspace.

\subsubsection{Formulation}
For a pre-trained weight matrix $\mathbf{W}_0 \in \mathbb{R}^{d \times k}$, LoRA introduces a low-rank decomposition to represent the update, $\Delta \mathbf{W}$. The modified weight matrix, $\mathbf{W}$, is then:
\begin{equation}
\mathbf{W} = \mathbf{W}_0 + \Delta \mathbf{W} = \mathbf{W}_0 + \mathbf{B}\mathbf{A}
\end{equation}
where $\mathbf{B} \in \mathbb{R}^{d \times r}$ and $\mathbf{A} \in \mathbb{R}^{r \times k}$, with the rank $r \ll \min(d, k)$. During fine-tuning, $\mathbf{W}_0$ is kept frozen, and only the matrices $\mathbf{A}$ and $\mathbf{B}$ are trained.

\subsubsection{Forward Pass Modification}
The forward pass is modified to incorporate the low-rank update. For an input $x$, the output $h$ is computed as:
\begin{equation}
h = \mathbf{W}_0 x + (\mathbf{B}\mathbf{A})x = \mathbf{W}_0 x + \mathbf{B}(\mathbf{A}x)
\end{equation}
The computation is grouped as $\mathbf{B}(\mathbf{A}x)$ to maintain computational efficiency.

\subsubsection{Initialization Strategy}
The LoRA matrices are carefully initialized to preserve the pre-trained model's performance at the beginning of fine-tuning. $\mathbf{A}$ is initialized with random values drawn from a Gaussian distribution, while $\mathbf{B}$ is initialized to all zeros. This ensures that the initial update, $\Delta \mathbf{W}$, is zero, and the model's behavior is identical to the pre-trained model at the start of the training process.

\subsubsection{Scaling Factor}
A scaling factor, $\alpha$, is often introduced to control the magnitude of the adaptation:
\begin{equation}
h = \mathbf{W}_0 x + \frac{\alpha}{r} (\mathbf{B}\mathbf{A})x
\end{equation}
Here, $\alpha$ is a hyperparameter that modulates the strength of the LoRA update. Setting $\alpha = r$ initially is a common practice, as it decouples the learning rate from the choice of rank $r$.

\subsubsection{Reduction in Trainable Parameters}
The number of trainable parameters is significantly reduced with LoRA. While full fine-tuning requires updating $d \times k$ parameters, LoRA only updates $r \times (d + k)$ parameters. For a typical transformer layer where $d \approx k$, the reduction factor is approximately $d / (2r)$. For instance, with $d=k=4096$ and a rank of $r=8$, LoRA achieves a 256-fold reduction in trainable parameters.

\subsubsection{Typical Target Modules}
LoRA is most commonly applied to the weight matrices within the attention mechanism, as these are often the most critical for task-specific adaptation. These include the query, key, value, and output projections ($W_q, W_k, W_v, W_o$).

\subsubsection{Inference Performance}
For inference, the trained LoRA matrices can be merged with the original weights:
\begin{equation}
\mathbf{W}_{\text{deployed}} = \mathbf{W}_0 + \frac{\alpha}{r} \mathbf{B}\mathbf{A}
\end{equation}
This means that LoRA introduces no additional latency during inference, as the adapter is fully absorbed into the base model.

\subsection{QLoRA: Quantized Low-Rank Adaptation}

QLoRA \cite{dettmers2024qlora} further reduces the memory footprint of fine-tuning by combining LoRA with quantization. It introduces three key innovations that enable the fine-tuning of extremely large models on a single GPU.

\subsubsection{Core Innovations of QLoRA}

4-bit NF4 Base Model Quantization\\
The frozen, pre-trained base model is quantized to the 4-bit NormalFloat (NF4) data type. During the forward pass, the weights are de-quantized on-the-fly to a higher precision (typically 16-bit BrainFloat, BF16) for computation, as shown in the equation $Y = \text{dequant}(W_{\text{NF4}}) \cdot X + (\mathbf{B}\mathbf{A})X$. This drastically reduces the memory required to store the base model, from 16 or 32 bits per parameter to approximately 4.5 bits per parameter (including quantization constants).
Double Quantization\\
To further reduce the memory overhead, QLoRA introduces "double quantization." The quantization constants themselves (e.g., the 32-bit scale factors for each group of weights) are also quantized. This second level of quantization typically uses an 8-bit representation, further reducing the memory footprint by approximately 0.3-0.5 bits per parameter.
Paged Optimizers\\
Fine-tuning with gradient checkpointing can still lead to memory spikes that exceed GPU capacity. QLoRA leverages paged optimizers, which use NVIDIA's unified memory feature to automatically page optimizer states to CPU RAM when GPU memory is exhausted. This process is transparent to the user and allows for the fine-tuning of much larger models than would otherwise be possible.

\subsubsection{Memory Footprint Comparison}
The following table illustrates the dramatic memory savings achieved by QLoRA for a 65-billion-parameter model:

\begin{table}[H]
\centering
\begin{tabular}{l|c|c}
\toprule
\textbf{Component} & \textbf{Full FT (BF16)} & \textbf{QLoRA} \\
\midrule
Base model weights & 260 GB & ~33 GB (4-bit) \\
Gradients & 260 GB & 0 (frozen base) \\
Optimizer states (Adam) & 520 GB & ~0.5 GB (LoRA only) \\
Activations (checkpointed) & Variable & ~4 GB \\
LoRA adapters & N/A & ~0.5 GB \\
\midrule
\textbf{Total (Approx.)} & \textbf{> 1 TB} & \textbf{~38 GB} \\
\bottomrule
\end{tabular}
\caption{Approximate memory requirements for fine-tuning a 65B model.}
\end{table}
