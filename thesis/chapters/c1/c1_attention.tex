\section{Attention Optimization Methodologies}

\subsection{FlashAttention: IO-Aware Exact Attention}

\subsubsection{Memory Hierarchy Context}
\begin{itemize}
    \item HBM (High Bandwidth Memory): 40-80GB, ~1.5-2 TB/s bandwidth
    \item SRAM (On-chip): ~20MB total (192KB x 108 SMs on A100), ~19 TB/s bandwidth
\end{itemize}

Standard attention materializes $N \times N$ intermediate matrices in HBM, causing memory-bound operations.

\subsubsection{Standard Attention Memory Pattern}
\begin{verbatim}
1. S = Q @ K^T          -> Write NxN to HBM
2. P = softmax(S)       -> Read NxN, Write NxN to HBM  
3. O = P @ V            -> Read NxN from HBM
Total HBM access: Theta(N^2 + Nd) 
\end{verbatim}

\subsubsection{FlashAttention Tiling Strategy}
Divide Q, K, V into blocks that fit in SRAM and compute attention incrementally:
\begin{verbatim}
Block sizes: B_r (row blocks), B_c (column blocks)
Constraint: B_r * B_c <= M (SRAM size)

For each K,V block j:
  For each Q block i:
    Load Q_i, K_j, V_j to SRAM
    Compute local attention: S_ij = Q_i @ K_j^T
    Apply local softmax with running statistics
    Update output accumulator
\end{verbatim}

\subsubsection{Online Softmax Algorithm}
The challenge: softmax requires the full row to compute the normalization constant.

Solution: Track running maximum $m$ and sum $l$, rescale incrementally:
\begin{verbatim}
Initialize: m = -inf, l = 0, O = 0

For each block j:
  1. Compute S_ij = Q_i @ K_j^T / sqrt(d)
  2. Compute block maximum: m_tilde = rowmax(S_ij)
  3. Update global maximum: m_new = max(m, m_tilde)
  4. Compute local softmax: P_tilde_ij = exp(S_ij - m_tilde)
  5. Update running sum: l_new = e^(m - m_new) * l + e^(m_tilde - m_new) * rowsum(P_tilde_ij)
  6. Update output: O = e^(m - m_new) * O + e^(m_tilde - m_new) * P_tilde_ij @ V_j
  7. m = m_new, l = l_new

Final: O = O / l
\end{verbatim}

\subsubsection{IO Complexity}
\begin{verbatim}
Standard attention: Theta(Nd + N^2) HBM accesses
FlashAttention: Theta(N^2*d^2/M) HBM accesses
\end{verbatim}

For typical M (SRAM size), this is a significant reduction.

\subsubsection{Backward Pass}
FlashAttention recomputes S and P during backward pass rather than storing them:
\begin{itemize}
    \item Saves $O(N^2)$ memory
    \item Additional FLOPs are offset by reduced memory traffic
    \item Stores only O (output) and softmax normalization statistics (m, l)
\end{itemize}

\subsection{PagedAttention: Virtual Memory for KV-Cache}

\subsubsection{Problem}
KV-cache grows dynamically and varies per request. Traditional pre-allocation wastes 60-80% of memory through:
\begin{itemize}
    \item Internal fragmentation: Pre-allocated slots never filled
    \item External fragmentation: Gaps between variable-size allocations
    \item Redundant duplication: Shared prefixes stored multiple times
\end{itemize}

\subsubsection{Core Concept}
Apply OS virtual memory principles:
\begin{itemize}
    \item Pages $\rightarrow$ KV blocks (fixed size, e.g., 16 tokens)
    \item Virtual addresses $\rightarrow$ Logical block indices
    \item Physical addresses $\rightarrow$ GPU memory locations
    \item Page table $\rightarrow$ Block table mapping logical to physical
\end{itemize}

\subsubsection{Data Structures}
\begin{minted}{python}
class BlockTable:
    # Maps logical_block_idx -> (physical_block_idx, num_filled_slots)
    entries: List[Tuple[int, int]]

class KVCacheManager:
    physical_blocks: Tensor  # [num_blocks, num_heads, head_dim, block_size]
    free_blocks: List[int]   # Available physical block indices
    block_tables: Dict[request_id, BlockTable]
\end{minted}

\subsubsection{Attention Computation}
\begin{minted}{python}
def paged_attention(query, block_table, k_cache, v_cache):
    output = zeros_like(query) 
    
    for logical_idx, (physical_idx, filled) in enumerate(block_table):
        # Fetch non-contiguous KV blocks
        k_block = k_cache[physical_idx, :, :, :filled]
        v_block = v_cache[physical_idx, :, :, :filled]
        
        # Compute attention for this block
        scores = query @ k_block.T / sqrt(head_dim)
        # Accumulate with proper softmax normalization
        output += softmax(scores) @ v_block
    
    return output
\end{minted}

\subsubsection{Memory Sharing (Copy-on-Write)}
\begin{verbatim}
Scenario: Multiple outputs from same prompt

1. All sequences share physical blocks for prompt KV-cache
2. Reference count tracks sharing: ref_count[physical_block] = num_sharers
3. On modification:
   if ref_count > 1:
       new_block = allocate_free_block()
       copy(physical_blocks[original], physical_blocks[new_block])
       update block_table entry
       ref_count[original] -= 1
\end{verbatim}

\subsubsection{Block Size Trade-offs}
\begin{itemize}
    \item Larger blocks $\rightarrow$ Better GPU utilization, more internal fragmentation
    \item Smaller blocks $\rightarrow$ Less waste, more overhead
    \item Typical: 16 tokens (balances ~4% waste vs good parallelism)
\end{itemize}
