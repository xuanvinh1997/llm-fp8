\section{Our Approach}\label{sec:approach}

This section presents our systematic approach to layer-wise FP8 format assignment for transformer training. Our methodology consists of three main components: computational pattern analysis, layer-wise format assignment strategy, and implementation architecture.

\subsection{Layer-Wise Format Assignment Strategy}

Based on our analysis of computational patterns in transformer components, we propose the following systematic assignment strategy:

\textbf{MLP Components:}
For all MLP layers $\ell \in \mathcal{L}_{\mathrm{MLP}}$, we assign weights, activations, and gradients to E4M3 format:
\begin{equation}
\forall \ell \in \mathcal{L}_{\mathrm{MLP}}: \quad W_{\ell}, A_{\ell}, G_{\ell} \mapsto \text{E4M3}
\end{equation}

\textbf{Attention Components:}
For attention layers $\ell \in \mathcal{L}_{\mathrm{Attn}}$, we apply mixed format assignment:
\begin{equation}
\forall \ell \in \mathcal{L}_{\mathrm{Attn}}: \quad
\begin{cases}
Q_{\ell}, K_{\ell} \mapsto \text{E5M2} \\
V_{\ell}, O_{\ell} \mapsto \text{E4M3} \\
G_{Q,\ell}, G_{K,\ell} \mapsto \text{E5M2} \\
G_{V,\ell}, G_{O,\ell} \mapsto \text{E4M3}
\end{cases}
\end{equation}

where $W$, $A$, and $G$ denote weights, activations, and gradients respectively, and $Q$, $K$, $V$, $O$ represent the standard attention projections.

This assignment strategy optimizes the precision-range trade-off for each component type:
\begin{itemize}
\item E4M3 for operations requiring high precision within moderate ranges (MLP operations, value projections)
\item E5M2 for operations requiring wide dynamic range coverage (query-key interactions, attention gradients)
\end{itemize}

\subsection{Implementation Architecture}

Our implementation leverages NVIDIA's Transformer Engine to provide seamless integration with existing PyTorch workflows. The architecture involves systematic module replacement where standard PyTorch layers (\texttt{nn.Linear}, \texttt{nn.LayerNorm}, \texttt{RMSNorm}) are replaced with their Transformer Engine equivalents that support format-specific FP8 computation.

The layer-wise format assignment is implemented through a two-stage process:
\begin{enumerate}
\item \textbf{Model Conversion:} Convert pre-trained model layers to Transformer Engine layers while preserving original parameters
\item \textbf{Format Configuration:} Apply layer-specific FP8 format configurations based on component type (MLP vs Attention)
\end{enumerate}

\subsection{Training Configuration}

We validate our approach on two model scales:
\begin{itemize}
\item \textbf{Llama-3.2-3B}: 3 billion parameters, 32 layers, 32 attention heads
\item \textbf{Llama-3.1-8B}: 8 billion parameters, 32 layers, 32 attention heads
\end{itemize}

Models are trained on 100K instruction-response pairs from the OpenMathInstruct-2 corpus for 3 epochs with sequence length of 512 tokens. We use AdamW optimizer with learning rate 1e-5, batch sizes optimized per model scale, and gradient accumulation of 4 steps.
