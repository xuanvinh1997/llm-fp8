% \section{Problem}

\textbf{Low-bit quantization} has emerged as a promising solution to shrink model size and speed up computation, making deployment of LLMs on edge hardware more practical. Quantization involves using reduced numerical precision to represent model weights and activations. By compressing model parameters from the standard 32-bit floating point (FP32) down to 8-bit or lower, we can dramatically cut memory usage and computational load. Recent advances in mixed-precision techniques even allow combining different precisions (e.g. 16-bit and 4-bit) in one matrix multiplication operation to balance speed and accuracy. Hardware vendors and researchers are actively exploring ultra-low precision arithmetic (int8, int4, even binary) to push LLMs onto resource-constrained environments.