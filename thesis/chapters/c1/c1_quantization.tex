\section{Quantization Methodologies}

\subsection{Mathematical Foundation of Quantization}

Quantization maps high-precision floating-point values to lower-precision representations. For a weight tensor $\mathbf{W}$, the fundamental quantization operation is:

\begin{equation*}\label{eq:quant_op}
Q(w) = \Delta \cdot \text{round}(w/\Delta + z)
\end{equation*}

Where:
\begin{itemize}
    \item $\Delta$ (delta) is the scale factor: $\Delta = (\max(W) - \min(W)) / (2^b - 1)$
    \item $z$ is the zero-point for asymmetric quantization
    \item $b$ is the target bit-width
\end{itemize}

The layer-wise quantization objective for LLMs minimizes reconstruction error:

\begin{equation*}\label{eq:quant_loss}
\text{argmin}_{\hat{W}} ||WX - \hat{W}X||^2_2
\end{equation*}

Where $W$ is the original weight matrix, $\hat{W}$ is the quantized weight matrix, and $X$ represents layer inputs from calibration data.

\subsection{GPTQ Algorithm: Optimal Brain Quantization for LLMs}

\subsubsection{Theoretical Foundation}
GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which itself derives from Optimal Brain Surgeon (OBS). The key insight is that quantizing a weight introduces error that can be partially compensated by adjusting remaining unquantized weights.

\subsubsection{The OBQ Update Formula}
When quantizing weight $w_q$, the optimal update $\delta_F$ for remaining weights $F$ is:

\begin{equation*}\label{eq:obq_update}
\delta_F = -w_q \cdot (H_F)^{-1} \cdot h_{Fq} / [H^{-1}]_{qq}
\end{equation*}

Where:
\begin{itemize}
    \item $H = XX^T + \lambda I$ is the Hessian matrix (with regularization $\lambda$)
    \item $h_{Fq}$ is the column of $H$ corresponding to the quantized weight
    \item $[H^{-1}]_{qq}$ is the diagonal element of the inverse Hessian
\end{itemize}

\subsubsection{GPTQ Key Innovations}

\begin{enumerate}
    \item \textbf{Arbitrary Order Quantization:} Unlike OBQ which greedily selects weights minimizing immediate error, GPTQ quantizes all rows in the same column order. Empirically, this produces equivalent results for large models while enabling massive parallelization.

    \item \textbf{Lazy Batch Updates:} Instead of updating the Hessian after each weight, GPTQ processes blocks of B columns (typically B=128):
    \begin{itemize}
        \item Quantize B columns using current Hessian information
        \item Batch-update remaining columns after processing the block
        \item This improves GPU utilization by converting memory-bound operations to compute-bound
    \end{itemize}

    \item \textbf{Cholesky Reformulation:} For numerical stability, GPTQ uses Cholesky decomposition of the inverse Hessian:
    \begin{equation*}\label{eq:cholesky_reformulation}
H^{-1} = (\text{Chol}(H^{-1}))^T \cdot \text{Chol}(H^{-1})
\end{equation*}
    This eliminates accumulating numerical errors during sequential processing.
\end{enumerate}

\subsubsection{Algorithm Pseudocode}
\begin{minted}{text}
Input: Weight matrix W, inverse Hessian H_inv, sparsity/quantization target
Output: Quantized weight matrix W_quant

1. Compute Cholesky decomposition of H_inv
2. For j = 0 to d_col in blocks of size B:
   a. For each column in block:
      - Quantize column j: w_quant_j = quant(w_j)
      - Compute quantization error: delta_j = (w_j - w_quant_j) / [H_inv]_jj
      - Update remaining columns in block: W[:,j+1:] -= delta_j * H_inv[j,j+1:]
   b. Apply lazy batch update to columns beyond current block
3. Return W_quant
\end{minted}

\subsubsection{Complexity}
$O(d_{\text{col}} \cdot d_{\text{row}} \cdot d_{\text{col}}) = O(d^3_{\text{col}})$ per layer, with the inverse Hessian computation dominating at $O(d^3_{\text{col}})$.

\subsection{AWQ: Activation-Aware Weight Quantization}

\subsubsection{Core Observation}
Not all weights contribute equally to model outputs. AWQ identifies that approximately 0.1-1% of weight channels—those corresponding to large activation magnitudes—are "salient" and protecting them significantly reduces quantization error.

\subsubsection{Salient Channel Identification}
Unlike magnitude-based selection, AWQ uses activation statistics:
\begin{equation*} Saliency(\text{channel}_i) = \text{mean}(|X[:,i]|) \cdot ||W[i,:]||
\end{equation*}
Channels with high saliency scores process more important features.

\subsubsection{Scaling-Based Protection}
Instead of mixed-precision (which is hardware-inefficient), AWQ scales salient channels before quantization:

For weight $w$ and input activation $x$, the output $y = w \cdot x$. If we scale:
\begin{itemize}
    \item $w' = s \cdot w$ (scale up weight)
    \item $x' = x / s$ (scale down activation)
\end{itemize}

The output remains unchanged: $y' = w' \cdot x' = (s\cdot w) \cdot (x/s) = w \cdot x = y$

\subsubsection{Quantization Error Analysis}
For a weight $w$ with scale factor $s$ and quantization step $\Delta$:
\begin{equation*} \text{Error}(w) = |w - Q(w)| \leq \Delta/2
\end{equation*}
\begin{equation*} \text{Error}(s\cdot w) \text{ scaled back} = |w - Q(s\cdot w)/s| \leq \Delta/(2s)
\end{equation*}

By choosing $s > 1$ for salient channels, the effective quantization error decreases proportionally.

\subsubsection{Optimal Scale Search}
AWQ searches for optimal per-channel scales using grid search:
\begin{equation*} s^* = \text{argmin}_s ||Q(s\cdot W)(X/s) - W\cdot X||^2
\end{equation*}

The search space is typically $s \in \{1, 1.25, 1.5, ..., 4\}$ for salient channels.

\subsubsection{Algorithm}
\begin{enumerate}
    \item Collect activation statistics from calibration data
    \item Identify salient channels (top 1% by activation magnitude)
    \item For each salient channel $i$:
    \begin{enumerate}
        \item Search for optimal scale $s_i$ minimizing reconstruction error
        \item Apply equivalent transformation: $W[i,:] *= s_i$
    \end{enumerate}
    \item Fuse inverse scales into adjacent layer (absorbed during inference)
    \item Perform group-wise quantization on transformed weights
\end{enumerate}

\subsubsection{Key Advantage}
AWQ requires no backpropagation or weight reconstruction, making it faster than GPTQ while achieving comparable or better quality.

\subsection{NF4 Quantization: Information-Theoretically Optimal Data Type}

\subsubsection{Motivation}
Pre-trained neural network weights follow approximately normal distributions. Standard uniform quantization levels waste representation capacity on regions with low probability density.

\subsubsection{NF4 Construction}
NF4 creates 16 quantization levels (4 bits) that are information-theoretically optimal for $N(0,1)$:

\begin{enumerate}
    \item Compute quantiles of the standard normal distribution:
    \begin{equation*}
    q_i = \Phi^{-1}(i/(2^k + 1)) \text{ for } i = 1, ..., 2^k - 1
    \end{equation*}
    Where $\Phi^{-1}$ is the inverse CDF of $N(0,1)$

    \item Center quantization bins between adjacent quantiles:
    \begin{equation*}
    c_i = (q_i + q_{i+1}) / 2
    \end{equation*}

    \item Ensure exact zero representation by creating asymmetric positive/negative ranges
\end{enumerate}

\subsubsection{NF4 Quantization Levels (normalized to [-1, 1])}
\begin{verbatim}
Negative: -1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0
Positive: 0.0, 0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230, 1.0
\end{verbatim}

\subsubsection{Quantization Procedure}
\begin{enumerate}
    \item Normalize weights to [-1, 1] using absolute maximum: $w_{\text{norm}} = w / \max(|W|)$
    \item Find nearest NF4 level: $q = \text{argmin}_i |w_{\text{norm}} - \text{nf4\_levels}[i]|$
    \item Store 4-bit index $q$ and scale factor $\max(|W|)$
\end{enumerate}
