\chapter{Experiments and Evaluation}
\label{chap:experiments_and_evaluation}

This chapter presents the empirical evaluation of our proposed layer-wise FP8 training methodology. We detail the datasets and configurations used in our experiments, and then present a comprehensive analysis of the results, focusing on performance, convergence, and model quality.

\section{Models and Datasets}
We selected two models from the Llama 2 family for our experiments: Llama-2-7B and Llama-2-13B. These models provide a representative basis for evaluating our methodology on current state-of-the-art architectures. For pre-training, we used a curated 100,000-sample subset of the RedPajama-Data-1T dataset, focusing on high-quality English text.

\section{Training Configuration}
The models were trained for three epochs using the AdamW optimizer with a learning rate of 1e-5. To manage memory constraints while maintaining a large effective batch size, we used a per-device batch size of 6 and a gradient accumulation of 4 steps. The key training parameters are summarized in Table \ref{tab:fp8_config}.

\begin{table}[h!]
    \centering
    \small
    \caption{Training configuration for Llama models.}
    \label{tab:fp8_config}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Configuration} & \textbf{Llama-2-7B} & \textbf{Llama-2-13B} \\
        \midrule
        Batch Size (per device) & 6 & 6 \\
        Gradient Accumulation & 4 & 4 \\
        Effective Batch Size & 24 & 24 \\
        Sequence Length & 512 & 512 \\
        Learning Rate & 1e-5 & 1e-5 \\
        Epochs & 3 & 3 \\
        Training Samples & 100K & 100K \\
        Optimizer & AdamW & AdamW \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Results}

\subsection{Memory Efficiency and Training Performance}
Our layer-wise FP8 approach demonstrates significant memory savings and performance gains. Table~\ref{tab:memory_results} presents memory utilization and training time across different model scales and precision configurations.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Precision} & \textbf{VRAM (GB)} & \textbf{Time (hours)} \\
\midrule
\multirow{3}{*}{Llama-3.2-3B} & BF16 & 82.4 & 3.6 \\
 & Hybrid FP8 & 74.2 & 2.2 \\
 & Ours (Layer-wise FP8) & \textbf{74.2} & \textbf{2.1} \\
\midrule
\multirow{3}{*}{Llama-3.1-8B} & BF16 & 80.0 & 9.1 \\
 & Hybrid FP8 & 88.0 & 7.6 \\
 & Ours (Layer-wise FP8) & 88.3 & \textbf{6.6} \\
\bottomrule
\end{tabular}
\caption{Memory utilization and training time across model scales and precision formats.}
\label{tab:memory_results}
\end{table}

For Llama-3.2-3B, our approach achieves a 10.0\% memory reduction compared to the BF16 baseline. For Llama-3.1-8B, we observe a slight increase in allocated VRAM, which is likely due to workspace and scaling-metadata overheads in the current implementation.

In terms of training time, our layer-wise FP8 method delivers substantial improvements. As shown in Figure~\ref{fig:training_time_comparison}, for the 3B model, we achieve a 42\% reduction in training time compared to BF16. The performance advantage is even more pronounced at the 8B scale, where our approach is 27\% faster than BF16 and 13\% faster than the Hybrid FP8 baseline.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/c4/training_time.png}
    \caption{Training time comparison across model configurations.}
    \label{fig:training_time_comparison}
\end{figure}

\subsection{Convergence and Model Quality}
Our experiments demonstrate that layer-wise FP8 training achieves comparable final loss and perplexity values to the BF16 baseline, while maintaining superior training dynamics.

Figure~\ref{fig:training_loss} shows that both BF16 and our FP8 approach converge to similar final loss values. However, our layer-wise format assignment produces smoother loss curves with reduced oscillations compared to the hybrid FP8 implementation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/c4/avg_loss.png}
    \caption{Average training loss comparison between BF16 and FP8 approaches.}
    \label{fig:training_loss}
\end{figure}

Similarly, Figure~\ref{fig:training_perplexity} shows that our layer-wise FP8 approach maintains training perplexity in the 1.30-1.32 range, demonstrating minimal degradation from the BF16 baseline and more stable perplexity trajectories compared to hybrid FP8 configurations.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/c4/train_perplexity.png}
    \caption{Training perplexity comparison across model scales and precision formats.}
    \label{fig:training_perplexity}
\end{figure}

\subsection{Numerical Stability}
A critical advantage of our layer-wise FP8 approach is its exceptional numerical stability. Figure~\ref{fig:stability_comparison} illustrates that our method maintains a loss variance consistently below 0.4 throughout training, which is approximately 50\% lower than the hybrid FP8 approach. This improved stability, which translates to more predictable training dynamics, is a direct result of our selective use of E5M2 for high-dynamic-range operations and E4M3 for more stable operations.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/c4/numeric_stability.png}
    \caption{Numerical stability analysis showing loss variance over training steps.}
    \label{fig:stability_comparison}
\end{figure}

\section{Analysis}
The results presented in this chapter demonstrate the effectiveness of our layer-wise FP8 training methodology. Our approach achieves significant performance gains and memory savings, particularly at larger model scales, without compromising model quality or numerical stability. The analysis of the results will be further discussed in the following chapter.