@article{bai2007skeleton,
  author    = {Xiang Bai and Longin Jan Latecki},
  title     = {Skeleton Pruning by Contour Partitioning with Discrete Curve Evolution},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2007},
  month     = {April},
  volume    = {29},
  number    = {3},
  pages     = {449-462},
  doi       = {10.1109/TPAMI.2007.59},
  source    = {PubMed}
}
@article{SHEN2011196,
title = {Skeleton growing and pruning with bending potential ratio},
journal = {Pattern Recognition},
volume = {44},
number = {2},
pages = {196-209},
year = {2011},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2010.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S0031320310004073},
author = {Wei Shen and Xiang Bai and Rong Hu and Hongyuan Wang and Longin {Jan Latecki}},
keywords = {Skeleton, Skeleton pruning, Skeleton growing, Bending potential ratio},
abstract = {We propose a novel significance measure for skeleton pruning, called bending potential ratio (BPR), in which the decision regarding whether a skeletal branch should be pruned or not is based on the context of the boundary segment that corresponds to the branch. By considering this contextual information, we can better evaluate the contribution of the boundary segment to the overall shape, which generally depends on its particular location within the whole contour (i.e., a segment may be considered to be insignificant in one place while it may be considered as a feature elsewhere). The BPR is a measure of the significance of contour segments in such context, and depicts the bending potential of a contour segment. Unlike other significance measures that only contain local shape information, the BPR evaluates both local and global shape information. Thus, it is insensitive to local boundary deformation. In addition, we also present a scheme for skeleton growing, which integrates pruning based on the BPR measurement. Our experiments demonstrate that the skeletons obtained by the proposed algorithm are medially placed and connected. We also demonstrate that shapes reconstructed from these skeletons are very close to the original shapes. Moreover, the BPR measure yields a natural multi-scale skeletal representation.}
}
@incollection{blum1967,
  author    = {H. Blum},
  title     = {A Transformation for Extracting New Descriptions of Shape},
  booktitle = {Models for the Perception of Speech and Visual Form},
  editor    = {W. Wathen-Dunn},
  publisher = {MIT Press},
  year      = {1967},
  pages     = {363-380},
}
@misc{fedus2022switchtransformersscalingtrillion,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03961}, 
}

@misc{shazeer2017outrageouslylargeneuralnetworks,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.06538}, 
}

@dataset{glaiveai_glaive_function_calling_v2,
  author = {GlaiveAI},
  title = {Glaive-Function-Calling-v2},
  year = {2023}, % You may need to update this based on the actual release year
  url = {https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2},
  publisher = {Hugging Face Datasets}
}

@misc{huggingface2024fp8,
  author = {Hugging Face},
  title = {Fine-grained FP8 Quantization},
  year = {2024},
  howpublished = {\url{https://huggingface.co/docs/transformers/quantization/finegrained_fp8}},
  note = {Accessed: March 22, 2025}
}

@misc{nvidia_fp8_primer,
  author       = {NVIDIA},
  title        = {Mixed Precision Training with FP8},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html}},
  note         = {Accessed April 6, 2025},
}

@inproceedings{Yadav_2019,
   title={Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering},
   url={http://dx.doi.org/10.18653/v1/D19-1260},
   DOI={10.18653/v1/d19-1260},
   booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   publisher={Association for Computational Linguistics},
   author={Yadav, Vikas and Bethard, Steven and Surdeanu, Mihai},
   year={2019} }
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018},
  note={Provides the ARC (AI2 Reasoning Challenge) dataset, including ARC Easy and ARC Challenge splits.}
}

@inproceedings{amini-etal-2019-mathqa,
    title = "{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    author = {Amini, Aida  and
      Gabriel, Saadia  and
      Lin, Shanchuan  and
      Sap, Maarten  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh  and
      Choi, Yejin},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1245",
    doi = "10.18653/v1/N19-1245",
    pages = "2357--2367",
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30961--30979},
  year={2022},
  note={Introduces the Minerva model and its evaluation, often performed on math datasets like GSM8K or MATH. The specific "Minerva Math" benchmark likely refers to the evaluation setup or a specific dataset used/curated in this context.}
}

@misc{huggingface,
  author =       {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Barham, Stephen and Hutchison, Alexander M. and Pickering, Quentin and Biderman, Stella and Gao, Leo and Townsend, Sean and Matrosov, Anton and Drame, Mariama and Lhoest, Quentin and Sutantar, Nathan and Lambert, Sylvain},
  title =        {Transformers: State-of-the-art Natural Language Processing},
  year =         {2019},
  howpublished = {\url{https://github.com/huggingface/transformers}},
}


@misc{transformer_engine,
    title = {Transformer Engine},
    author = {NVIDIA},
    howpublished = {\url{https://github.com/NVIDIA/TransformerEngine}},
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}