% ==== FP8 and Mixed Precision Training ====

@misc{micikevicius2022fp8formatsdeeplearning,
      title={FP8 Formats for Deep Learning},
      author={Paulius Micikevicius and Dusan Stosic and Neil Burgess and Marius Cornea and Pradeep Dubey and Richard Grisenthwaite and Sangwon Ha and Alexander Heinecke and Patrick Judd and John Kamalu and Naveen Mellempudi and Stuart Oberman and Mohammad Shoeybi and Michael Siu and Hao Wu},
      year={2022},
      eprint={2209.05433},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.05433},
}

@misc{nvidia_fp8_primer,
  author       = {NVIDIA},
  title        = {Mixed Precision Training with FP8},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html}},
  year         = {2025},
  note         = {Accessed April 6, 2025},
}

@inproceedings{narang2017mixed,
      title={Mixed precision training},
      author={Narang, Sharan and Diamos, Gregory and Sengupta, Shubho and Elsen, Erich},
      booktitle={International Conference on Learning Representations},
      year={2018}
}

@inproceedings{micikevicius2018mixed,
      title={Mixed precision training},
      author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
      booktitle={International Conference on Learning Representations},
      year={2018}
}

@misc{kalamkar2019study,
      title={A Study of BFloat16 for Deep Learning Training},
      author={Dhiraj Kalamkar and Dheevatsa Mudigere and Naveen Mellempudi and Dipankar Das and Kunal Banerjee and Sasikanth Avancha and others},
      year={2019},
      eprint={1905.12322},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.12322}
}

% ==== Quantization Methods ====

@inproceedings{jacob2018quantization,
      title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
      author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
      booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
      pages={2704--2713},
      year={2018}
}

@inproceedings{dettmers2022gpt3,
      title={GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
      author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
      booktitle={Advances in Neural Information Processing Systems},
      volume={35},
      pages={30318--30332},
      year={2022}
}

% ==== Large Language Models ====

@misc{deepseekv3,
      title={DeepSeek-V3 Technical Report},
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437},
}

@misc{meta2024llama3.2,
      title={Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
      author={Meta AI},
      year={2024},
      month={September},
      howpublished={\url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}},
      note={Meta AI Blog, September 25, 2024}
}

@misc{meta2024llama3.1,
      title={Introducing Llama 3.1: Our most capable models to date},
      author={Meta AI},
      year={2024},
      month={July},
      howpublished={\url{https://ai.meta.com/blog/meta-llama-3-1/}},
      note={Meta AI Blog, July 23, 2024}
}

% ==== Transformer Architecture ====

@inproceedings{vaswani2017attention,
      title={Attention is all you need},
      author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
      booktitle={Advances in Neural Information Processing Systems},
      volume={30},
      pages={5998--6008},
      year={2017}
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer},
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202},
}

% ==== Datasets ====

@misc{toshniwal2024openmath2,
      title={OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data},
      author={Shubham Toshniwal and Wei Du and Ivan Moshkov and Branislav Kisacanin and Alexan Ayrapetyan and Igor Gitman},
      year={2024},
      eprint={2410.01560},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.01560},
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018},
  note={Provides the ARC (AI2 Reasoning Challenge) dataset, including ARC Easy and ARC Challenge splits.}
}

@inproceedings{amini-etal-2019-mathqa,
    title = "{M}ath{QA}: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    author = {Amini, Aida  and
      Gabriel, Saadia  and
      Lin, Shanchuan  and
      Sap, Maarten  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh  and
      Choi, Yejin},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1245",
    doi = "10.18653/v1/N19-1245",
    pages = "2357--2367",
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30961--30979},
  year={2022},
  note={Introduces the Minerva model and its evaluation, often performed on math datasets like GSM8K or MATH. The specific "Minerva Math" benchmark likely refers to the evaluation setup or a specific dataset used/curated in this context.}
}

% ==== Tools and Frameworks ====

@misc{transformer_engine,
    title = {Transformer Engine},
    author = {NVIDIA},
    year = {2025},
    howpublished = {\url{https://github.com/NVIDIA/TransformerEngine}},
    note = {NVIDIA Deep Learning Documentation}
}

@misc{huggingface,
  author =       {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Barham, Stephen and Hutchison, Alexander M. and Pickering, Quentin and Biderman, Stella and Gao, Leo and Townsend, Sean and Matrosov, Anton and Drame, Mariama and Lhoest, Quentin and Sutantar, Nathan and Lambert, Sylvain},
  title =        {Transformers: State-of-the-art Natural Language Processing},
  year =         {2019},
  howpublished = {\url{https://github.com/huggingface/transformers}},
}

@misc{huggingface2024fp8,
  author = {Hugging Face},
  title = {Fine-grained FP8 Quantization},
  year = {2024},
  howpublished = {\url{https://huggingface.co/docs/transformers/quantization/finegrained_fp8}},
  note = {Accessed: March 22, 2025}
}

% ==== Mixture of Experts ====

@misc{fedus2022switchtransformersscalingtrillion,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03961},
}

@misc{shazeer2017outrageouslylargeneuralnetworks,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1701.06538},
}
