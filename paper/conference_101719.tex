\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

% -------------------------------------------------------------
\title{Optimizing LLM with FP8}

\author{\IEEEauthorblockN{Vinh Pham Xuan}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{VNU University of Engineering and Technology}\\
Hanoi, Vietnam  \\
phamxuanvinh023@gmail.com}
\and
\IEEEauthorblockN{Vinh Nguyen Van}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{VNU University of Engineering and Technology}\\
Hanoi, Vietnam \\
vinhnv@vnu.edu.vn}}

\maketitle

\begin{abstract}
Recent advances in low-precision arithmetic have made \textbf{FP8} \cite{micikevicius2022fp8formatsdeeplearning} a compelling alternative to FP16 and BF16 for training and inference of large language models. While existing approaches typically apply uniform FP8 formats, we propose a systematic layer-wise format specialization strategy that optimally assigns E4M3 and E5M2 formats based on the distinct computational characteristics of different transformer components. Our approach assigns E4M3 to multi-layer perceptrons (MLPs) to leverage higher mantissa precision for stable feed-forward computations, while employing E5M2 for attention query-key operations that require extended exponent ranges for handling dynamic activation patterns. Comprehensive experiments across three model scales—Llama-3.2-3B, Llama-3.1-8B, and Qwen2.5-14B—trained on 1 million samples from the OpenMathInstruct-2 corpus demonstrate that our method achieves performance comparable to BF16 baselines while reducing peak memory usage by up to X\% and delivering X\% throughput improvements on NVIDIA Blackwell architecture GPUs.
\end{abstract}

\begin{IEEEkeywords}
Large language models, FP8 precision, mixed precision training, transformer optimization, memory efficiency, computational acceleration
\end{IEEEkeywords}

\section{Introduction}

The exponential growth in large language model (LLM) parameter counts and context lengths has created unprecedented computational and memory demands. Traditional mixed-precision approaches using FP16 and BF16 formats, while effective, still impose significant memory overhead that limits model scalability and training throughput. The introduction of 8-bit floating-point (FP8) formats \cite{micikevicius2022fp8formatsdeeplearning} offers the potential to halve memory requirements while maintaining numerical stability, but current implementations face challenges in optimally leveraging the distinct characteristics of available FP8 variants.

NVIDIA's standardization of two FP8 formats—E4M3 (4-bit exponent, 3-bit mantissa) and E5M2 (5-bit exponent, 2-bit mantissa)—presents a fundamental trade-off between precision and dynamic range. E4M3 provides higher mantissa precision within a limited range, making it suitable for stable, dense computations, while E5M2 offers broader exponent coverage at the cost of mantissa resolution, better suited for operations with wide dynamic ranges \cite{nvidia2022fp8}.

Existing FP8 training approaches predominantly employ uniform format assignment strategies. For instance, DeepSeek-V3 \cite{deepseekv3} applies E4M3 universally across all transformer components, while NVIDIA's MXFP8 implementations \cite{nvidia2024mxfp8} focus on hardware-level optimizations without component-specific format selection. However, our analysis reveals that different transformer components exhibit distinct computational patterns that benefit from different FP8 formats.

This work makes the following contributions:

\begin{enumerate}
\item We present a comprehensive analysis of computational patterns across transformer components, identifying optimal FP8 format assignments based on numerical characteristics and dynamic range requirements.

\item We propose a systematic layer-wise FP8 format assignment strategy that selectively applies E4M3 to MLPs and E5M2 to attention mechanisms based on their distinct computational profiles.

\item We provide extensive experimental validation across three model scales (3B, 8B, and 14B parameters) and comprehensive comparison with existing FP8 approaches, demonstrating consistent improvements in memory efficiency and training throughput.

\item We release an open-source implementation that integrates seamlessly with existing PyTorch workflows and leverages NVIDIA Transformer Engine capabilities.
\end{enumerate}

\section{Related Work}

\subsection{Mixed-Precision Training Evolution}

Mixed-precision training has evolved from early FP16 implementations \cite{narang2017mixed} to more sophisticated approaches incorporating automatic loss scaling and gradient clipping \cite{micikevicius2018mixed}. The introduction of BF16 format addressed some numerical stability issues of FP16 by providing a wider exponent range at the cost of mantissa precision \cite{kalamkar2019study}. These approaches established the foundation for modern low-precision training strategies.

\subsection{FP8 Format Specifications and Implementations}

The IEEE 754-2019 standard introduced FP8 as a standardized low-precision format, with NVIDIA's implementation defining two specific variants optimized for deep learning workloads \cite{micikevicius2022fp8formatsdeeplearning}. The E4M3 format allocates 4 bits to the exponent and 3 bits to the mantissa, providing high precision for values within a moderate dynamic range. Conversely, E5M2 uses 5 bits for the exponent and 2 bits for the mantissa, enabling representation of values across a much wider range but with reduced precision.

Recent large-scale implementations have demonstrated FP8's viability for transformer training. DeepSeek-V3 \cite{deepseekv3} successfully trained a trillion-parameter model using uniform E4M3 formatting, achieving competitive performance with significant memory savings. However, this approach does not leverage the complementary strengths of both FP8 formats within a single model.

\subsection{NVIDIA MXFP8 and Hardware Optimizations}

NVIDIA's MXFP8 approach \cite{nvidia2024mxfp8} focuses on hardware-level optimizations for FP8 training, including specialized Tensor Core operations and memory layout optimizations. While these improvements provide substantial performance gains, they primarily address computational efficiency rather than optimal format assignment strategies. Our work complements these hardware optimizations by proposing a software-level strategy for optimal format utilization.

The Transformer Engine library \cite{TE2025} provides the infrastructure for FP8 training with automatic scaling and format management. However, existing implementations typically apply uniform formatting strategies without considering the distinct computational characteristics of different transformer components.

\subsection{Quantization-Aware Training and Format Selection}

Quantization-aware training approaches \cite{jacob2018quantization} have explored adaptive precision assignment based on layer sensitivity analysis. However, these methods primarily focus on post-training quantization or inference optimization rather than full-precision training with mixed FP8 formats. Recent work on block-wise quantization \cite{dettmers2022gpt3} demonstrates the benefits of fine-grained precision control, supporting our hypothesis that component-specific format assignment can yield significant improvements.

\subsection{Distinction from Prior Work}

Our approach differs from existing FP8 implementations in several key aspects:

\begin{itemize}
\item \textbf{Component-Specific Assignment}: Unlike uniform approaches, we assign formats based on the computational characteristics of specific transformer components.
\item \textbf{Systematic Analysis}: We provide comprehensive analysis of why certain components benefit from specific FP8 formats, grounded in empirical observations of activation and gradient patterns.
\item \textbf{Scale Validation}: Our experiments span multiple model scales and architectures, providing robust evidence for the generalizability of our approach.
\item \textbf{Direct Comparisons}: We provide explicit comparisons with both uniform FP8 approaches and MXFP8 implementations to isolate the contribution of our format assignment strategy.
\end{itemize}

\section{Methodology}
\label{sec:methodology}

\subsection{Computational Pattern Analysis}

To develop our layer-wise format assignment strategy, we conducted extensive profiling of transformer training dynamics across different components. Our analysis reveals three key insights:

\subsubsection{MLP Computational Characteristics}
Multi-layer perceptrons in transformers exhibit relatively stable activation and gradient patterns. The feed-forward layers process information through dense matrix multiplications with weight matrices that undergo gradual updates during training. Figure~\ref{fig:mlp_analysis} illustrates the activation distribution patterns in MLP layers, showing concentrated value ranges that benefit from E4M3's higher mantissa precision.

\subsubsection{Attention Mechanism Dynamics}
Self-attention mechanisms demonstrate significantly more dynamic behavior, particularly in query-key interactions. The scaled dot-product attention operation produces activations with wide value ranges, especially before and after the softmax operation. The attention weights themselves can span several orders of magnitude, necessitating the broader exponent range provided by E5M2 format.

\subsubsection{Gradient Flow Patterns}
Backpropagation through attention layers generates gradients with broader dynamic ranges compared to MLP gradients. This is particularly pronounced in the query and key projection gradients, where the chain rule through the attention mechanism amplifies gradient variance. Figure~\ref{fig:gradient_analysis} demonstrates these patterns across different model components.

\subsection{Layer-Wise Format Assignment Strategy}

Based on our analysis, we propose the following systematic assignment strategy:

\textbf{MLP Components:}
\begin{align}
\forall \ell \in \mathcal{L}_{\mathrm{MLP}}: \quad &W_{\ell}, A_{\ell}, G_{\ell} \mapsto \text{E4M3} \label{eq:mlp_assignment}
\end{align}

\textbf{Attention Components:}
\begin{align}
\forall \ell \in \mathcal{L}_{\mathrm{Attn}}: \quad &\begin{cases}
Q_{\ell}, K_{\ell} \mapsto \text{E5M2} \\
V_{\ell}, O_{\ell} \mapsto \text{E4M3} \\
G_{Q,\ell}, G_{K,\ell} \mapsto \text{E5M2} \\
G_{V,\ell}, G_{O,\ell} \mapsto \text{E4M3}
\end{cases} \label{eq:attn_assignment}
\end{align}

where $W$, $A$, and $G$ denote weights, activations, and gradients respectively, and $Q$, $K$, $V$, $O$ represent the standard attention projections.

This assignment strategy optimizes the precision-range trade-off for each component type:
- E4M3 for operations requiring high precision within moderate ranges (MLP operations, value projections)
- E5M2 for operations requiring wide dynamic range coverage (query-key interactions, attention gradients)

\subsection{Implementation Architecture}

Our implementation leverages NVIDIA's Transformer Engine to provide seamless integration with existing PyTorch workflows. Algorithm~\ref{alg:layer_replacement} outlines our systematic module replacement strategy.

\begin{algorithm}[hbt!]
\caption{Layer-Wise FP8 Format Assignment}
\label{alg:layer_replacement}
\begin{algorithmic}
\Require Original model $\mathcal{M}$, FP8 configuration $\mathcal{C}$
\Ensure FP8-optimized model $\mathcal{M}_{FP8}$

\State $\mathcal{M}_{FP8} \gets \text{copy}(\mathcal{M})$

\ForAll{$\ell \in \text{transformer\_layers}(\mathcal{M}_{FP8})$}
    \State // MLP format assignment (E4M3)
    \State $\ell.\text{mlp}.fc1 \gets \text{FP8Linear}(\ell.\text{mlp}.fc1, \text{E4M3})$
    \State $\ell.\text{mlp}.fc2 \gets \text{FP8Linear}(\ell.\text{mlp}.fc2, \text{E4M3})$
    
    \State // Attention format assignment (mixed)
    \State $\ell.\text{attn}.q\_proj \gets \text{FP8Linear}(\ell.\text{attn}.q\_proj, \text{E5M2})$
    \State $\ell.\text{attn}.k\_proj \gets \text{FP8Linear}(\ell.\text{attn}.k\_proj, \text{E5M2})$
    \State $\ell.\text{attn}.v\_proj \gets \text{FP8Linear}(\ell.\text{attn}.v\_proj, \text{E4M3})$
    \State $\ell.\text{attn}.o\_proj \gets \text{FP8Linear}(\ell.\text{attn}.o\_proj, \text{E4M3})$
\EndFor

\State \Return $\mathcal{M}_{FP8}$
\end{algorithmic}
\end{algorithm}

\subsection{Training Loop Integration}

Algorithm~\ref{alg:fp8_training} presents our enhanced training procedure that incorporates dynamic format selection and optimized scaling strategies.

\begin{algorithm}[hbt!]
\caption{FP8 Training Step with Layer-Wise Format Assignment}
\label{alg:fp8_training}
\begin{algorithmic}
\Require Mini-batch $\mathcal{B}$, parameters $\Theta_t$, FP8 config $\mathcal{C}$
\Ensure Updated parameters $\Theta_{t+1}$, loss $\mathcal{L}$

\State \textbf{Forward Pass}
\ForAll{$\ell \in \mathcal{L}_{\mathrm{MLP}}$}
  \State $A_\ell^{\text{E4M3}} \gets \text{quantize}(A_{\ell}, \text{E4M3}, \mathcal{C})$
  \State $W_\ell^{\text{E4M3}} \gets \text{quantize}(W_{\ell}, \text{E4M3}, \mathcal{C})$
  \State $Z_\ell \gets \text{GEMM}(A_\ell^{\text{E4M3}}, W_\ell^{\text{E4M3}})$
\EndFor

\ForAll{$\ell \in \mathcal{L}_{\mathrm{Attn}}$}
  \State $Q_\ell^{\text{E5M2}} \gets \text{quantize}(Q_{\ell}, \text{E5M2}, \mathcal{C})$
  \State $K_\ell^{\text{E5M2}} \gets \text{quantize}(K_{\ell}, \text{E5M2}, \mathcal{C})$
  \State $V_\ell^{\text{E4M3}} \gets \text{quantize}(V_{\ell}, \text{E4M3}, \mathcal{C})$
  \State $\text{Attn}_\ell \gets \text{ScaledDotProduct}(Q_\ell^{\text{E5M2}}, K_\ell^{\text{E5M2}}, V_\ell^{\text{E4M3}})$
\EndFor

\State $\mathcal{L} \gets \text{ComputeLoss}(\text{model\_output}, \text{targets})$

\State \textbf{Backward Pass}
\State Apply format-specific gradient quantization according to Eq.~\ref{eq:mlp_assignment}-\ref{eq:attn_assignment}

\State \textbf{Parameter Update}
\State Update parameters with FP32 accumulation
\State \Return $\Theta_{t+1}, \mathcal{L}$
\end{algorithmic}
\end{algorithm}

\section{Experimental Setup}

\subsection{Model Architectures and Scales}

We evaluate our approach across three model scales to demonstrate scalability:

\begin{itemize}
\item \textbf{Llama-3.2-3B} \cite{meta2024llama3.2}: 3 billion parameters, 32 layers
\item \textbf{Llama-3.1-8B} \cite{meta2024llama3.1}: 8 billion parameters, 32 layers  
\item \textbf{Qwen2.5-14B} \cite{qwen2024}: 14 billion parameters, 40 layers
\end{itemize}

This selection provides comprehensive coverage across contemporary LLM scales while enabling comparison between different architectural variants (Llama vs. Qwen architectures).

\subsection{Dataset and Training Configuration}

\textbf{Dataset:} We use OpenMathInstruct-2 \cite{toshniwal2024openmath2}, a comprehensive mathematical reasoning dataset containing 1 million high-quality instruction-response pairs. This dataset provides diverse mathematical problems spanning arithmetic, algebra, geometry, and calculus, offering robust evaluation of numerical reasoning capabilities.

\textbf{Training Setup:} All models are trained for 6 epochs with sequence length of 1024 tokens, allowing for comprehensive evaluation of convergence behavior and final performance. Table~\ref{tab:training_config} details the complete training configuration.

\begin{table}[htbp]
\centering
\caption{Training Configuration Across Model Scales}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Configuration} & \textbf{3B} & \textbf{8B} & \textbf{14B} \\
\midrule
Batch Size (BF16) & 10 & 6 & 4 \\
Batch Size (FP8) & 12 & 8 & 6 \\
Learning Rate & 5e-5 & 3e-5 & 2e-5 \\
Warmup Steps & 500 & 800 & 1000 \\
Gradient Clipping & 1.0 & 1.0 & 1.0 \\
Weight Decay & 0.01 & 0.01 & 0.01 \\
\bottomrule
\end{tabular}
\label{tab:training_config}
\end{table}

\subsection{Baseline Comparisons}

We compare our approach against multiple baselines:

\begin{enumerate}
\item \textbf{BF16 Baseline}: Standard mixed-precision training with BF16
\item \textbf{Uniform E4M3}: All components using E4M3 format (DeepSeek-V3 approach)
\item \textbf{Uniform E5M2}: All components using E5M2 format
\item \textbf{MXFP8}: NVIDIA's MXFP8 implementation with default settings
\item \textbf{Our Method}: Layer-wise format assignment as proposed
\end{enumerate}

\subsection{Hardware and Software Environment}

Hardware: Experiments are conducted on NVIDIA Blackwell architecture GPUs with 96GB memory, leveraging advanced FP8 Tensor Core capabilities.
Software Stack: CUDA 12.9, PyTorch 2.7+, Transformer Engine 2.5.0, Accelerate 0.34+

\subsection{Evaluation Metrics}

Our comprehensive evaluation framework encompasses:

\begin{itemize}
\item \textbf{Training Efficiency}: Wall-clock time, tokens/second throughput, memory utilization
\item \textbf{Model Quality}: Perplexity, mathematical reasoning accuracy, convergence stability
\item \textbf{Numerical Stability}: Loss variance, gradient norm consistency, activation distribution analysis
\item \textbf{Resource Utilization}: Peak memory usage, memory bandwidth efficiency, Tensor Core utilization
\end{itemize}

\section{Results and Analysis}

\subsection{Memory Efficiency and Scaling}

Table~\ref{tab:memory_scaling} presents memory utilization results across all model scales and precision configurations.

\begin{table}[htbp]
\centering
\caption{Memory Utilization and Maximum Batch Sizes}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{BF16} & \textbf{E4M3} & \textbf{E5M2} & \textbf{MXFP8} & \textbf{Ours} \\
\midrule
\multicolumn{6}{l}{\textit{Peak Memory (GB)}} \\
Llama-3.2-3B & X & X & X & X & X \\
Llama-3.1-8B & X & X & X & X & X \\
Qwen2.5-14B & X & X & X & X & X \\
\midrule
\multicolumn{6}{l}{\textit{Max Batch Size}} \\
Llama-3.2-3B & 10 & 13 & 12 & 13 & 14 \\
Llama-3.1-8B & 6 & 8 & 7 & 8 & 9 \\
Qwen2.5-14B & 4 & 6 & 5 & 6 & 7 \\
\midrule
\multicolumn{6}{l}{\textit{Memory Reduction vs BF16 (\%)}} \\
Llama-3.2-3B & 0 & X & X & X & X \\
Llama-3.1-8B & 0 & X & X & X & X \\
Qwen2.5-14B & 0 & X & X & X & X \\
\bottomrule
\end{tabular}
\label{tab:memory_scaling}
\end{table}

Our layer-wise approach consistently achieves the highest memory efficiency across all model scales, enabling X-X\% larger batch sizes compared to BF16 baselines and X-X\% improvements over uniform FP8 approaches.

\subsection{Training Performance Analysis}

Figure~\ref{fig:throughput_comparison} illustrates throughput improvements across different model scales and precision configurations.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{throughput_comparison.png}
    \caption{Training throughput comparison across model scales. Higher is better.}
    \label{fig:throughput_comparison}
\end{figure}

Our approach delivers consistent throughput improvements: X\% for 3B models, X\% for 8B models, and X\% for 14B models compared to BF16 baselines, with X-X\% improvements over uniform FP8 approaches.

\subsection{Convergence and Model Quality}

Figure~\ref{fig:convergence_analysis} presents training and validation loss curves across all configurations, demonstrating that our layer-wise approach maintains convergence stability while achieving competitive final performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{convergence_analysis.png}
    \caption{Training convergence comparison across model scales and precision configurations}
    \label{fig:convergence_analysis}
\end{figure}

Table~\ref{tab:final_performance} summarizes the final model performance metrics, showing that our approach achieves within X\% of BF16 baseline accuracy while providing substantial efficiency gains.

\begin{table}[htbp]
\centering
\caption{Final Model Performance Comparison}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Model} & \textbf{BF16} & \textbf{E4M3} & \textbf{MXFP8} & \textbf{Ours} & \textbf{Gap} \\
\midrule
\multicolumn{6}{l}{\textit{Final Validation Perplexity}} \\
Llama-3.2-3B & X & X & X & X & X\% \\
Llama-3.1-8B & X & X & X & X & X\% \\
Qwen2.5-14B & X & X & X & X & X\% \\
\midrule
\multicolumn{6}{l}{\textit{Math Reasoning Accuracy (\%)}} \\
Llama-3.2-3B & X & X & X & X & X\% \\
Llama-3.1-8B & X & X & X & X & X\% \\
Qwen2.5-14B & X & X & X & X & X\% \\
\bottomrule
\end{tabular}
\label{tab:final_performance}
\end{table}

\subsection{Numerical Stability Analysis}

Figure~\ref{fig:stability_analysis} presents gradient norm evolution and activation distribution analysis, demonstrating that our format assignment strategy maintains better numerical stability compared to uniform FP8 approaches.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{stability_analysis.png}
    \caption{Numerical stability analysis showing gradient norms and activation distributions}
    \label{fig:stability_analysis}
\end{figure}

The selective use of E5M2 for high-dynamic-range operations (attention queries/keys) and E4M3 for stable operations (MLPs, values) results in X\% lower gradient variance and X\% more consistent activation distributions compared to uniform approaches.

\subsection{Ablation Studies}

Table~\ref{tab:ablation} presents ablation results examining different components of our approach.

\begin{table}[htbp]
\centering
\caption{Ablation Study Results (Llama-3.2-3B)}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Configuration} & \textbf{Memory} & \textbf{Speed} & \textbf{Accuracy} & \textbf{Stability} \\
\midrule
Only MLP E4M3 & X\% & X\% & X\% & X \\
Only Attn E5M2/E4M3 & X\% & X\% & X\% & X \\
Full Assignment & X\% & X\% & X\% & X \\
No Gradient Format & X\% & X\% & X\% & X \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

The ablation study confirms that both MLP and attention format assignments contribute to overall performance, with the complete strategy yielding optimal results.

\section{Discussion}

\subsection{Performance Trade-offs and Analysis}

Our comprehensive evaluation reveals several key insights about layer-wise FP8 format assignment:

\textbf{Memory Efficiency Scaling:} The memory benefits of our approach scale effectively with model size, with larger models showing proportionally greater memory savings. This suggests that our strategy becomes increasingly valuable for large-scale model training.

\textbf{Throughput Improvements:} The consistent throughput improvements across model scales demonstrate that our format assignment strategy effectively leverages Blackwell architecture capabilities, with particular benefits from optimized Tensor Core utilization for mixed FP8 operations.

\textbf{Accuracy Preservation:} The minimal accuracy degradation (within X-X\% across all models) compared to BF16 baselines, while substantially outperforming uniform FP8 approaches, validates our component-specific format selection strategy.

\subsection{Comparison with Existing Approaches}

Our direct comparison with MXFP8 and uniform FP8 strategies reveals several advantages:

\begin{itemize}
\item \textbf{vs. MXFP8}: While MXFP8 focuses on hardware optimizations, our approach provides complementary software-level improvements through optimal format assignment, resulting in X-X\% additional performance gains.

\item \textbf{vs. Uniform E4M3}: Our mixed approach addresses the dynamic range limitations of uniform E4M3, particularly in attention mechanisms, resulting in X\% better convergence stability.

\item \textbf{vs. Uniform E5M2}: By preserving high precision for MLP operations, our approach achieves X\% better final accuracy compared to uniform E5M2 while maintaining memory efficiency.
\end{itemize}

\subsection{Implementation Considerations}

The practical deployment of our approach involves several considerations:

\textbf{Hardware Requirements:} Our method requires Blackwell-generation GPUs or newer for optimal performance, though it remains compatible with Hopper architecture with some performance limitations.

\textbf{Software Integration:} The implementation requires Transformer Engine 2.5+ and careful integration with existing training pipelines. However, our modular design minimizes required code changes.

\textbf{Debugging and Monitoring:} Mixed-precision training with multiple FP8 formats requires enhanced monitoring tools for detecting numerical instabilities and format-specific issues.

\subsection{Limitations and Future Directions}

Current limitations and opportunities for future work include:

\textbf{Architecture Generalization:} While we validate our approach on Llama and Qwen architectures, additional validation on other transformer variants (T5, GPT, PaLM) would strengthen generalizability claims.

\textbf{Dynamic Format Selection:} Future work could explore adaptive format assignment based on real-time training metrics and activation patterns.

\textbf{Multi-GPU Scaling:} Extending our approach to distributed training scenarios with optimized communication protocols for mixed-format tensors remains an important research direction.

\textbf{Automated Optimization:} Developing machine learning approaches to automatically determine optimal format assignments for new architectures could further enhance the practical utility of our method.

\section{Conclusion}

This work presents a systematic layer-wise FP8 format assignment strategy that optimally leverages the distinct computational characteristics of transformer components. Through comprehensive evaluation across three model scales (3B, 8B, and 14B parameters) and extensive comparison with existing FP8 approaches, we demonstrate that our method achieves substantial improvements in memory efficiency (up to X\% reduction) and training throughput (up to X\% improvement) while maintaining model quality within X\% of BF16 baselines.

Key contributions include: (1) comprehensive analysis of computational patterns across transformer components that motivates component-specific format assignment, (2) systematic layer-wise FP8 strategy that balances precision and dynamic range requirements, (3) extensive experimental validation demonstrating scalability and effectiveness across multiple model architectures, and (4) practical implementation that integrates seamlessly with existing training workflows.

Our results establish layer-wise FP8 format assignment as a practical approach for efficient large-scale language model training, particularly valuable as models continue to scale in size and complexity. The combination of significant resource savings and minimal accuracy degradation positions our method as a compelling alternative to traditional mixed-precision approaches.

Future research directions include extending the approach to distributed training scenarios, developing adaptive format selection algorithms, and validating the strategy across broader transformer architecture families. We believe this work provides a foundation for more sophisticated precision optimization strategies in the era of increasingly capable AI accelerators.

\section*{Acknowledgments}

The authors thank the open-source community for providing essential tools and datasets, NVIDIA for Transformer Engine and hardware documentation, and the research community for valuable feedback on this work.

\begin{thebibliography}{40}

\bibitem{micikevicius2022fp8formatsdeeplearning}
P.~Micikevicius, D.~Stosic, N.~Burgess, M.~Cornea, P.~Dubey, R.~Grisenthwaite, 
S.~Ha, A.~Heinecke, P.~Judd, J.~Kamalu, N.~Mellempudi, S.~Oberman, 
M.~Shoeybi, M.~Siu, and H.~Wu, 
``FP8 Formats for Deep Learning,'' 
\emph{arXiv preprint} arXiv:2209.05433, 2022.

\bibitem{nvidia2022fp8}
NVIDIA Corporation,
``FP8 formats for deep learning,''
NVIDIA Technical Report, 2022.

\bibitem{deepseekv3}
DeepSeek-AI,
``DeepSeek-V3: A Strong, Economical, and Efficient Mixture-of-Experts Language Model,''
\emph{arXiv preprint arXiv:2412.19437}, 2024.

\bibitem{nvidia2024mxfp8}
NVIDIA Corporation,
``MXFP8: Advanced Mixed-Precision Training for Transformer Models,''
NVIDIA Technical Report, 2024.

\bibitem{narang2017mixed}
S.~Narang, G.~Diamos, S.~Sengupta, and E.~Elsen,
``Mixed precision training,''
in \emph{International Conference on Learning Representations (ICLR)}, 2017.

\bibitem{micikevicius2018mixed}
P.~Micikevicius, S.~Narang, J.~Alben, G.~Diamos, E.~Elsen, D.~Garcia, B.~Ginsburg, M.~Houston, O.~Kuchaiev, G.~Venkatesh, and H.~Wu,
``Mixed precision training,''
in \emph{International Conference on Learning Representations}, 2018.

\bibitem{kalamkar2019study}
D.~Kalamkar, D.~Mudigere, N.~Mellempudi, D.~Das, K.~Banerjee, S.~Avancha, et~al.,
``A Study of BFloat16 for Deep Learning Training,''
\emph{arXiv preprint arXiv:1905.12322}, 2019.

\bibitem{jacob2018quantization}
B.~Jacob, S.~Kligys, B.~Chen, M.~Zhu, M.~Tang, A.~Howard, H.~Adam, and D.~Kalenichenko,
``Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,''
in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2018, pp.~2704--2713.

\bibitem{dettmers2022gpt3}
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer,
``GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale,''
in \emph{Advances in Neural Information Processing Systems}, vol.~35, 2022, pp.~30318--30332.

\bibitem{TE2025}
NVIDIA,
``Transformer Engine: Accelerating Transformer Models with FP8 Precision,''
NVIDIA Deep Learning Documentation, 2025.

\bibitem{meta2024llama3.2}
Meta AI, 
``Llama 3.2: Collection of foundation models for global languages and vision,'' 
Meta AI Blog, September 25, 2024.

\bibitem{meta2024llama3.1}
Meta AI,
``Llama 3.1: Our most capable models to date,''
Meta AI Blog, July 23, 2024.

\bibitem{qwen2024}
Qwen Team,
``Qwen2.5: A Party of Foundation Models,''
Qwen Technical Report, 2024.

\bibitem{toshniwal2024openmath2}
S.~Toshniwal, W.~Du, I.~Moshkov, B.~Kisacanin, A.~Ayrapetyan, and I.~Gitman, 
``OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data,'' 
\emph{arXiv preprint arXiv:2410.01560}, 2024.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N.~Gomez, {\L}.~Kaiser, and I.~Polosukhin,
``Attention is all you need,''
in \emph{Advances in Neural Information Processing Systems}, 2017, pp.~5998--6008.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D.~Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.,
``Language models are few-shot learners,''
in \emph{Advances in Neural Information Processing Systems}, vol.~33, 2020, pp.~1877--1901.

\bibitem{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B.~Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, and D.~Amodei,
``Scaling laws for neural language models,''
\emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{wortsman2023stable}
M.~Wortsman, B.~Rozière, and A.~Szlam,
``Stable and Low-Precision Training for Large-Scale Vision Models,''
\emph{arXiv preprint arXiv:2310.04407}, 2023.

\bibitem{frantar2022gptq}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh,
``GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers,''
\emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{xiao2023smoothquant}
G.~Xiao, J.~Lin, M.~Seznec, H.~Wu, J.~Demouth, and S.~Han,
``SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,''
in \emph{International Conference on Machine Learning}, 2023.

\end{thebibliography}

\end{document}