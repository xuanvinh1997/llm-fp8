\chapter{Introduction}
Large Language Models (LLMs) have achieved remarkable capabilities across a range of tasks, from natural language understanding to complex reasoning. However, this prowess comes with steep resource demands.

State-of-the-art models often consist of billions of parameters, requiring huge memory and computational power for inference and training. For example, a 70B-parameter model like LLaMA-2 (70B) occupies around 140 GB in full precision, exceeding the RAM of typical edge devices. 

Running such models on edge devices (smartphones, laptops, embedded systems) is challenging or infeasible without compression, as insufficient memory leads to slow performance or crashes. This limitation prevents many real-world use cases and restricts access to advanced AI on ubiquitous devices.

\input{chapters/c1/c1_mo_overview}
\input{chapters/c1/c1_mo_problem_definition}

\input{chapters/c1/c1_mo_dataset}
\input{chapters/c1/c1_mo_approach}
% \input{chapters/c1/c1_application}
