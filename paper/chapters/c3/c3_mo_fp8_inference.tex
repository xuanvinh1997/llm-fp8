\section{Inference}\label{sec:inference}

The fine-tuned checkpoint is exported from training and loaded into \texttt{vLLM}, whose paged-attention kernel and speculative decoding yield high-throughput text generation. Before the model goes into production, we benchmark its behavior in three numerical formats natively supported on Hopper GPUs. The first setting, \textbf{FP8}, preserves the precision used during training (E4M3 weights paired with E5M2 gradients). The second, \textbf{FP16}, serves as a mainstream half-precision baseline and is obtained by up-casting the FP8 weights once the model is loaded. The third, \textbf{FP32}, provides a full-precision reference highlighting the efficiency gains delivered by reduced-bit execution.

All three variants run on the same NVIDIA H100 NVL with identical batch size and sequence length. During each test, we record two quantities. The primary figure of merit is token throughput, measured in tokens per second and averaged across ten runs that omit warm-up passes to minimize timing jitter. We also log peak device memory in gibibytes using \texttt{nvidia-smi}; this value reflects the total RAM footprint of the serving stack, including activation buffers and attention caches.

The results, summarised in Section~\ref{sec:time_reduction}, show that the FP8 configuration delivers the fastest generation while consuming the least memory, confirming that the precision advantages observed during training extend to inference. After validating these numbers, we deploy the FP8 checkpoint to the production cluster and expose it through a RESTful API endpoint for downstream applications.
